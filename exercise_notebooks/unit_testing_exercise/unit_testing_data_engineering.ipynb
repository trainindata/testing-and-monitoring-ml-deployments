{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing ML Code: Hands-on Exercise (Data Engineering)\n",
    "\n",
    "## In this notebook we will explore unit tests for data engineering\n",
    "\n",
    "#### We will use a classic toy dataset: the Iris plants dataset, which comes included with scikit-learn\n",
    "Dataset details: https://scikit-learn.org/stable/datasets/index.html#iris-plants-dataset\n",
    "\n",
    "As we progress through the course, the complexity of examples will increase, but we will start with something basic. This notebook is designed so that it can be run in isolation, once the setup steps described below are complete.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Let's begin by importing the dataset and the libraries we are going to use. Make sure you have run `pip install -r requirements.txt` on requirements file located in the same directory as this notebook. We recommend doing this in a separate virtual environment (see dedicated setup lecture).\n",
    "\n",
    "If you need a refresher on jupyter, pandas or numpy, there are some links to resources in the section notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Access the iris dataset from sklearn\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Load the iris data into a pandas dataframe. The `data` and `feature_names`\n",
    "# attributes of the dataset are added by default by sklearn. We use them to\n",
    "# specify the columns of our dataframes.\n",
    "iris_frame = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Create a \"target\" column in our dataframe, and set the values to the correct\n",
    "# classifications from the dataset.\n",
    "iris_frame['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the `SimplePipeline` from the Test Input Values notebook (same as previous lecture, no changes here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class SimplePipeline:\n",
    "    def __init__(self):\n",
    "        self.frame = None\n",
    "        # Shorthand to specify that each value should start out as\n",
    "        # None when the class is instantiated.\n",
    "        self.X_train, self.X_test, self.y_train, self.Y_test = None, None, None, None\n",
    "        self.model = None\n",
    "        self.load_dataset()\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load the dataset and perform train test split.\"\"\"\n",
    "        # fetch from sklearn\n",
    "        dataset = datasets.load_iris()\n",
    "        \n",
    "        # remove units ' (cm)' from variable names\n",
    "        self.feature_names = [fn[:-5] for fn in dataset.feature_names]\n",
    "        self.frame = pd.DataFrame(dataset.data, columns=self.feature_names)\n",
    "        self.frame['target'] = dataset.target\n",
    "        \n",
    "        # we divide the data set using the train_test_split function from sklearn, \n",
    "        # which takes as parameters, the dataframe with the predictor variables, \n",
    "        # then the target, then the percentage of data to assign to the test set, \n",
    "        # and finally the random_state to ensure reproducibility.\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.frame[self.feature_names], self.frame.target, test_size=0.65, random_state=42)\n",
    "        \n",
    "    def train(self, algorithm=LogisticRegression):\n",
    "        \n",
    "        # we set up a LogisticRegression classifier with default parameters\n",
    "        self.model = algorithm(solver='lbfgs', multi_class='auto')\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        return self.model.predict(input_data)\n",
    "        \n",
    "    def get_accuracy(self):\n",
    "        \n",
    "        # use our X_test and y_test values generated when we used\n",
    "        # `train_test_split` to test accuracy.\n",
    "        # score is a method on the Logisitic Regression that \n",
    "        # returns the accuracy by default, but can be changed to other metrics, see: \n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score\n",
    "        return self.model.score(X=self.X_test, y=self.y_test)\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Helper method to run multiple pipeline methods with one call.\"\"\"\n",
    "        self.load_dataset()\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Engineered Data (preprocessing)\n",
    "\n",
    "Below we create an updated pipeline which inherits from the SimplePipeline but has new functionality to preprocess the data by applying a scaler. Linear models are sensitive to the scale of the features. For example features with bigger magnitudes tend to dominate if we do not apply a scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class PipelineWithDataEngineering(SimplePipeline):\n",
    "    def __init__(self):\n",
    "        # Call the inherited SimplePipeline __init__ method first.\n",
    "        super().__init__()\n",
    "        \n",
    "        # scaler to standardize the variables in the dataset\n",
    "        self.scaler = StandardScaler()\n",
    "        # Train the scaler once upon pipeline instantiation:\n",
    "        # Compute the mean and standard deviation based on the training data\n",
    "        self.scaler.fit(self.X_train)\n",
    "    \n",
    "    def apply_scaler(self):\n",
    "        # Scale the test and training data to be of mean 0 and of unit variance\n",
    "        self.X_train = self.scaler.transform(self.X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        # apply scaler transform on inputs before predictions\n",
    "        scaled_input_data = self.scaler.transform(input_data)\n",
    "        return self.model.predict(scaled_input_data)\n",
    "                  \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Helper method to run multiple pipeline methods with one call.\"\"\"\n",
    "        self.load_dataset()\n",
    "        self.apply_scaler()  # updated in the this class\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model accuracy is: 0.9591836734693877\n"
     ]
    }
   ],
   "source": [
    "pipeline = PipelineWithDataEngineering()\n",
    "pipeline.run_pipeline()\n",
    "accuracy_score = pipeline.get_accuracy()\n",
    "print(f'current model accuracy is: {accuracy_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we Unit Test\n",
    "We focus specifically on the feature engineering step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestIrisDataEngineering(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.pipeline = PipelineWithDataEngineering()\n",
    "        self.pipeline.load_dataset()\n",
    "    \n",
    "    def test_scaler_preprocessing_brings_x_train_mean_near_zero(self):\n",
    "        # Given\n",
    "        # convert the dataframe to be a single column with pandas stack\n",
    "        original_mean = self.pipeline.X_train.stack().mean()\n",
    "        \n",
    "        # When\n",
    "        self.pipeline.apply_scaler()\n",
    "        \n",
    "        # Then\n",
    "        # The idea behind StandardScaler is that it will transform your data \n",
    "        # to center the distribution at 0 and scale the variance at 1.\n",
    "        # Therefore we test that the mean has shifted to be less than the original\n",
    "        # and close to 0 using assertAlmostEqual to check to 3 decimal places:\n",
    "        # https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertAlmostEqual\n",
    "        self.assertTrue(original_mean > self.pipeline.X_train.mean())  # X_train is a numpy array at this point.\n",
    "        self.assertAlmostEqual(self.pipeline.X_train.mean(), 0.0, places=3)\n",
    "        print(f'Original X train mean: {original_mean}')\n",
    "        print(f'Transformed X train mean: {self.pipeline.X_train.mean()}')\n",
    "        \n",
    "    def test_scaler_preprocessing_brings_x_train_std_near_one(self):\n",
    "        # When\n",
    "        self.pipeline.apply_scaler()\n",
    "        \n",
    "        # Then\n",
    "        # We also check that the standard deviation is close to 1\n",
    "        self.assertAlmostEqual(self.pipeline.X_train.std(), 1.0, places=3)\n",
    "        print(f'Transformed X train standard deviation : {self.pipeline.X_train.std()}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X train mean: 3.5889423076923075\n",
      "Transformed X train mean: -5.978123978750843e-17\n",
      "Transformed X train standard deviation : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.029s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestIrisDataEngineering)\n",
    "unittest.TextTestRunner(verbosity=1, stream=sys.stderr).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Test: Hands-on Exercise\n",
    "Change the pipeline class preprocessing so that the test fails. Do you understand why the test is failing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

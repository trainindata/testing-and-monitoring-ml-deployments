{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing ML Code: Hands-on Exercise (Model Quality)\n",
    "\n",
    "## In this notebook we will explore unit tests for *model prediction quality*\n",
    "\n",
    "#### We will use a classic toy dataset: the Iris plants dataset, which comes included with scikit-learn\n",
    "Dataset details: https://scikit-learn.org/stable/datasets/index.html#iris-plants-dataset\n",
    "\n",
    "As we progress through the course, the complexity of examples will increase, but we will start with something basic. This notebook is designed so that it can be run in isolation, once the setup steps described below are complete. Cells should be run one after the other without skipping any.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Let's begin by importing the dataset and the libraries we are going to use. Make sure you have run `pip install -r requirements.txt` on requirements file located in the same directory as this notebook. We recommend doing this in a separate virtual environment (see dedicated setup lecture).\n",
    "\n",
    "If you need a refresher on jupyter, pandas or numpy, there are some links to resources in the section notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Access the iris dataset from sklearn\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Load the iris data into a pandas dataframe. The `data` and `feature_names`\n",
    "# attributes of the dataset are added by default by sklearn. We use them to\n",
    "# specify the columns of our dataframes.\n",
    "iris_frame = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Create a \"target\" column in our dataframe, and set the values to the correct\n",
    "# classifications from the dataset.\n",
    "iris_frame['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pipelines\n",
    "\n",
    "Below we use both pipelines from the previous exercises:\n",
    "\n",
    "- `SimplePipeline` from the testing inputs lecture\n",
    "- `PipelineWithFeatureEngineering` from the testing data engineering lecture\n",
    "\n",
    "The pipelines have not been changed. We use both so that we can compare predictions between them in our tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class SimplePipeline:\n",
    "    def __init__(self):\n",
    "        self.frame = None\n",
    "        # Shorthand to specify that each value should start out as\n",
    "        # None when the class is instantiated.\n",
    "        self.X_train, self.X_test, self.y_train, self.Y_test = None, None, None, None\n",
    "        self.model = None\n",
    "        self.load_dataset()\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load the dataset and perform train test split.\"\"\"\n",
    "        # fetch from sklearn\n",
    "        dataset = datasets.load_iris()\n",
    "        \n",
    "        # remove units ' (cm)' from variable names\n",
    "        self.feature_names = [fn[:-5] for fn in dataset.feature_names]\n",
    "        self.frame = pd.DataFrame(dataset.data, columns=self.feature_names)\n",
    "        self.frame['target'] = dataset.target\n",
    "        \n",
    "        # we divide the data set using the train_test_split function from sklearn, \n",
    "        # which takes as parameters, the dataframe with the predictor variables, \n",
    "        # then the target, then the percentage of data to assign to the test set, \n",
    "        # and finally the random_state to ensure reproducibility.\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.frame[self.feature_names], self.frame.target, test_size=0.65, random_state=42)\n",
    "        \n",
    "    def train(self, algorithm=LogisticRegression):\n",
    "        \n",
    "        # we set up a LogisticRegression classifier with default parameters\n",
    "        self.model = algorithm(solver='lbfgs', multi_class='auto')\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        return self.model.predict(input_data)\n",
    "        \n",
    "    def get_accuracy(self):\n",
    "        \n",
    "        # use our X_test and y_test values generated when we used\n",
    "        # `train_test_split` to test accuracy.\n",
    "        # score is a method on the Logisitic Regression that \n",
    "        # returns the accuracy by default, but can be changed to other metrics, see: \n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score\n",
    "        return self.model.score(X=self.X_test, y=self.y_test)\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Helper method to run multiple pipeline methods with one call.\"\"\"\n",
    "        self.load_dataset()\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class PipelineWithDataEngineering(SimplePipeline):\n",
    "    def __init__(self):\n",
    "        # Call the inherited SimplePipeline __init__ method first.\n",
    "        super().__init__()\n",
    "        \n",
    "        # scaler to standardize the variables in the dataset\n",
    "        self.scaler = StandardScaler()\n",
    "        # Train the scaler once upon pipeline instantiation:\n",
    "        # Compute the mean and standard deviation based on the training data\n",
    "        self.scaler.fit(self.X_train)\n",
    "    \n",
    "    def apply_scaler(self):\n",
    "        # Scale the test and training data to be of mean 0 and of unit variance\n",
    "        self.X_train = self.scaler.transform(self.X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        # apply scaler transform on inputs before predictions\n",
    "        scaled_input_data = self.scaler.transform(input_data)\n",
    "        return self.model.predict(scaled_input_data)\n",
    "                  \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Helper method to run multiple pipeline methods with one call.\"\"\"\n",
    "        self.load_dataset()\n",
    "        self.apply_scaler()  # updated in the this class\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we Unit Test\n",
    "\n",
    "We will employ a few different tests for model prediction quality:\n",
    "\n",
    "1. A benchmark test: checking model accuracy against a simple benchmark\n",
    "2. A differential test: checking model accuracy from one version to the next\n",
    "\n",
    "To begin, let's establish a base line. The simplest baseline is predicting the most common class. If we run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    50\n",
       "1    50\n",
       "0    50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_frame['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there an equal number of classifications for the 3 flower types. Let's check the accuracy when always predicting classification 1. Obviously this is a very low benchmark (circa 33% accuracy on the dataset), but it serves to illustrate the sort of checks you should be running with your models. If this test fails, then our model accuracy is terrible and we have probably introduced a severe bug into our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "\n",
    "class TestIrisPredictions(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # We prepare both pipelines for use in the tests\n",
    "        self.pipeline_v1 = SimplePipeline()\n",
    "        self.pipeline_v2 = PipelineWithDataEngineering()\n",
    "        self.pipeline_v1.run_pipeline()\n",
    "        self.pipeline_v2.run_pipeline()\n",
    "        \n",
    "        # the benchmark is simply the same classification value for \n",
    "        # for every test entry\n",
    "        self.benchmark_predictions = [1.0] * len(self.pipeline_v1.y_test)\n",
    "    \n",
    "    def test_accuracy_higher_than_benchmark(self):\n",
    "        # Given\n",
    "        benchmark_accuracy = accuracy_score(\n",
    "            y_true=self.pipeline_v1.y_test,\n",
    "            y_pred=self.benchmark_predictions)\n",
    "        \n",
    "        predictions = self.pipeline_v1.predict(self.pipeline_v1.X_test)\n",
    "        \n",
    "        # When\n",
    "        actual_accuracy = accuracy_score(\n",
    "            y_true=self.pipeline_v1.y_test,\n",
    "            y_pred=predictions)\n",
    "        \n",
    "        # Then\n",
    "        print(f'model accuracy: {actual_accuracy}, benchmark accuracy: {benchmark_accuracy}')\n",
    "        self.assertTrue(actual_accuracy > benchmark_accuracy)\n",
    "        \n",
    "    def test_accuracy_compared_to_previous_version(self):\n",
    "        # When\n",
    "        v1_accuracy = self.pipeline_v1.get_accuracy()\n",
    "        v2_accuracy = self.pipeline_v2.get_accuracy()\n",
    "        print(f'pipeline v1 accuracy: {v1_accuracy}')\n",
    "        print(f'pipeline v2 accuracy: {v2_accuracy}')\n",
    "        \n",
    "        # Then\n",
    "        self.assertTrue(v2_accuracy >= v1_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline v1 accuracy: 0.9693877551020408\n",
      "pipeline v2 accuracy: 0.9591836734693877\n",
      "model accuracy: 0.9693877551020408, benchmark accuracy: 0.32653061224489793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAIL: test_accuracy_compared_to_previous_version (__main__.TestIrisPredictions)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-ad175269f46a>\", line 42, in test_accuracy_compared_to_previous_version\n",
      "    self.assertTrue(v2_accuracy >= v1_accuracy)\n",
      "AssertionError: False is not true\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.115s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestIrisPredictions)\n",
    "unittest.TextTestRunner(verbosity=1, stream=sys.stderr).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quality Testing: Hands-on Exercise\n",
    "1. Change the SimplePipeline class so that the benchmark test fails. Do you understand why the test is failing?\n",
    "\n",
    "2. Change either the SimplePipeline or the PipelineWithDataEngineering classes so that `test_accuracy_compared_to_previous_version` **passes**. \n",
    "\n",
    "These tests are a little more open ended than others we have looked at, don't worry if you find them tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
